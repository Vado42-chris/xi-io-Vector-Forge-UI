# ‚úÖ Cut the Cord Setup Complete

## üéØ What Was Configured

This PC has been configured with the same "cut the cord" features as Loki-PC:

### 1. ‚úÖ Internal Intranet Setup
- **internal.xi-io.com** ‚Üí `127.0.0.1` (localhost)
- Added to `/etc/hosts` file
- Access VectorForge at: `http://internal.xi-io.com:3000`

### 2. ‚úÖ Offline Configuration
- **`.env.local`** created with offline settings
- **`config/xibalba.config.json`** updated for offline mode
- **Standalone mode** enabled
- **Local AI only** mode enabled

### 3. ‚úÖ Local AI Setup
- **Ollama** configuration ready
- **MCP Server** configuration ready
- **Fallback** to HTTP API if MCP unavailable

## üìã Configuration Files

### `.env.local`
```env
# Xibalba MCP Server (Local)
VITE_XIBALBA_MCP_URL=http://localhost:8000
VITE_XIBALBA_API_KEY=
VITE_XIBALBA_MODEL=xibalba-local

# Local AI (Ollama)
VITE_LOCAL_AI_URL=http://localhost:11434
VITE_LOCAL_AI_MODEL=codellama:latest

# Offline Mode
VITE_OFFLINE_MODE=true
VITE_STANDALONE_MODE=true

# Internal Intranet
VITE_INTERNAL_URL=http://internal.xi-io.com:3000
```

### `config/xibalba.config.json`
```json
{
  "installation": {
    "portable": true,
    "usbReady": true,
    "requiresInternet": false,
    "localAIOnly": true,
    "cutTheCord": true,
    "offlineMode": true,
    "standaloneMode": true
  }
}
```

### `/etc/hosts`
```
127.0.0.1    internal.xi-io.com
```

## üöÄ Next Steps

### 1. Start Ollama (if not running)
```bash
ollama serve
```

### 2. Pull a Model (if needed)
```bash
ollama pull codellama:latest
# or
ollama pull deepseek-coder:latest
```

### 3. Start VectorForge
```bash
npm run dev
```

### 4. Access the App
- **Internal Intranet**: `http://internal.xi-io.com:3000`
- **Localhost**: `http://localhost:3000`

## ‚úÖ Features Now Available

### Offline AI Processing
- ‚úÖ Local Ollama integration
- ‚úÖ MCP server support
- ‚úÖ HTTP API fallback
- ‚úÖ No internet required

### Standalone Operation
- ‚úÖ Local file system operations
- ‚úÖ Local terminal execution
- ‚úÖ Self-contained development environment
- ‚úÖ Works completely offline

### Portable Configuration
- ‚úÖ Environment variables (`.env.local`)
- ‚úÖ localStorage fallback
- ‚úÖ Default values
- ‚úÖ USB deployment ready

## üîß Verification

### Check Configuration
```bash
# Check hosts file
cat /etc/hosts | grep "internal.xi-io.com"

# Check environment variables
cat .env.local

# Check Ollama
curl http://localhost:11434/api/tags

# Check MCP server (optional)
curl http://localhost:8000/health
```

### Test Offline Mode
1. **Disconnect from internet** (or block external connections)
2. **Start VectorForge**: `npm run dev`
3. **Access**: `http://internal.xi-io.com:3000`
4. **Test AI features** - Should work with local Ollama

## üìù What's Different from Loki-PC

If something doesn't work, compare with Loki-PC:

1. **Ollama Models**: Check which models Loki-PC has
   ```bash
   # On Loki-PC:
   ollama list
   ```

2. **MCP Server**: Check if Loki-PC has a custom MCP server
   ```bash
   # On Loki-PC:
   curl http://localhost:8000/health
   ```

3. **Environment Variables**: Compare `.env.local` files
   ```bash
   # On Loki-PC:
   cat .env.local
   ```

4. **Network Setup**: Check if Loki-PC has additional network config
   ```bash
   # On Loki-PC:
   cat /etc/hosts
   ip route show
   ```

## üéâ Success Criteria

The setup is complete when:
- ‚úÖ `internal.xi-io.com` resolves to localhost
- ‚úÖ `.env.local` exists with offline config
- ‚úÖ `config/xibalba.config.json` has offline mode enabled
- ‚úÖ Ollama is running (or can be started)
- ‚úÖ VectorForge starts without errors
- ‚úÖ App is accessible at `http://internal.xi-io.com:3000`

## üîÑ Re-run Setup

If you need to re-run the setup:
```bash
./scripts/setup-cut-the-cord.sh
```

This script is idempotent - safe to run multiple times.

---

**Status**: ‚úÖ Cut the Cord features configured and ready!

